{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1 - Exercise 3: First Step with MLP\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the structure of nn.Linear layers (input and output dimensions)\n",
    "- Learn how to use basic activation functions (ReLU, Sigmoid, Tanh)\n",
    "- Build simple neural networks using nn.Sequential\n",
    "- Calculate the number of parameters in a neural network\n",
    "- Perform forward pass operations through the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Framework Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the test repository\n",
    "!git clone https://github.com/racousin/data_science_practice.git /tmp/tests 2>/dev/null || true\n",
    "\n",
    "# Import required modules\n",
    "import sys\n",
    "sys.path.append('/tmp/tests/tests/python_deep_learning')\n",
    "\n",
    "# Import the improved test utilities\n",
    "from test_utils import NotebookTestRunner, create_inline_test\n",
    "from module1.test_exercise3 import Exercise3Validator, EXERCISE3_SECTIONS\n",
    "\n",
    "# Create test runner and validator\n",
    "test_runner = NotebookTestRunner(\"module1\", 3)\n",
    "validator = Exercise3Validator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device(\"cuda\")   # <- one-liner\n",
    "    print(\"Default device:\", torch.empty(0).device)  # sanity check\n",
    "else:\n",
    "    print(\"CUDA not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Understanding nn.Linear\n",
    "\n",
    "The `nn.Linear` layer is the fundamental building block of MLPs. It performs a linear transformation: `y = xW^T + b`\n",
    "where x is the input, W is the weight matrix, and b is the bias vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear layer: Linear(in_features=10, out_features=5, bias=True)\n",
      "Weight shape: torch.Size([5, 10])\n",
      "Bias shape: torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create a linear layer that transforms input from 10 features to 5 features\n",
    "linear_layer_1 = nn.Linear(10, 5, device=\"cuda\")\n",
    "\n",
    "# Display layer information\n",
    "if linear_layer_1 is not None:\n",
    "    print(f\"Linear layer: {linear_layer_1}\")\n",
    "    print(f\"Weight shape: {linear_layer_1.weight.shape}\")\n",
    "    print(f\"Bias shape: {linear_layer_1.bias.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear layer 2: Linear(in_features=5, out_features=2, bias=True)\n",
      "Calculated parameters: 12\n",
      "Actual parameters: 12\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create a linear layer that transforms 5 features to 3 features\n",
    "linear_layer_2 = nn.Linear(5, 2, device=\"cuda\")\n",
    "\n",
    "# TODO: Calculate the total number of parameters in linear_layer_2\n",
    "# Remember: parameters = (input_size * output_size) + bias_size\n",
    "num_params_layer2 = 5 * 2 + 2 \n",
    "\n",
    "if linear_layer_2 is not None and num_params_layer2 is not None:\n",
    "    print(f\"Linear layer 2: {linear_layer_2}\")\n",
    "    print(f\"Calculated parameters: {num_params_layer2}\")\n",
    "    actual_params = sum(p.numel() for p in linear_layer_2.parameters())\n",
    "    print(f\"Actual parameters: {actual_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 1: Understanding nn.Linear\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE3_SECTIONS[\"Section 1: Understanding nn.Linear\"]]\n",
    "test_runner.test_section(\"Section 1: Understanding nn.Linear\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Activation Functions\n",
    "\n",
    "Activation functions introduce non-linearity into neural networks, allowing them to learn complex patterns.\n",
    "- **ReLU**: f(x) = max(0, x) - Most commonly used\n",
    "- **Sigmoid**: f(x) = 1/(1+e^(-x)) - Outputs between 0 and 1\n",
    "- **Tanh**: f(x) = (e^x - e^(-x))/(e^x + e^(-x)) - Outputs between -1 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([-2., -1.,  0.,  1.,  2.])\n",
      "ReLU output: tensor([0., 0., 0., 1., 2.])\n",
      "Sigmoid output: tensor([0.1192, 0.2689, 0.5000, 0.7311, 0.8808])\n",
      "Tanh output: tensor([-0.9640, -0.7616,  0.0000,  0.7616,  0.9640])\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create instances of the three main activation functions\n",
    "relu_activation = nn.ReLU()\n",
    "sigmoid_activation = nn.Sigmoid()\n",
    "tanh_activation = nn.Tanh()\n",
    "\n",
    "# Test the activations with sample input\n",
    "test_input = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "\n",
    "if relu_activation is not None:\n",
    "    print(f\"Input: {test_input}\")\n",
    "    print(f\"ReLU output: {relu_activation(test_input)}\")\n",
    "if sigmoid_activation is not None:\n",
    "    print(f\"Sigmoid output: {sigmoid_activation(test_input)}\")\n",
    "if tanh_activation is not None:\n",
    "    print(f\"Tanh output: {tanh_activation(test_input)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10])\n",
      "Linear output shape: torch.Size([2, 5])\n",
      "Activated output shape: torch.Size([2, 5])\n",
      "Number of negative values before ReLU: 9\n",
      "Number of negative values after ReLU: 0\n"
     ]
    }
   ],
   "source": [
    "# TODO: Apply ReLU activation to the output of linear_layer_1\n",
    "# First create some input data\n",
    "input_data = torch.randn(2, 10, device=\"cuda\")  # Batch size 2, 10 features\n",
    "\n",
    "# TODO: Pass input_data through linear_layer_1\n",
    "linear_output = linear_layer_1(input_data)\n",
    "\n",
    "# TODO: Apply ReLU activation to linear_output\n",
    "activated_output = relu_activation(linear_output)\n",
    "\n",
    "if linear_output is not None and activated_output is not None:\n",
    "    print(f\"Input shape: {input_data.shape}\")\n",
    "    print(f\"Linear output shape: {linear_output.shape}\")\n",
    "    print(f\"Activated output shape: {activated_output.shape}\")\n",
    "    print(f\"Number of negative values before ReLU: {(linear_output < 0).sum().item()}\")\n",
    "    print(f\"Number of negative values after ReLU: {(activated_output < 0).sum().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 2: Activation Functions\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE3_SECTIONS[\"Section 2: Activation Functions\"]]\n",
    "test_runner.test_section(\"Section 2: Activation Functions\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Building Networks with nn.Sequential\n",
    "\n",
    "`nn.Sequential` allows us to stack layers and create a neural network pipeline. The output of each layer becomes the input to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple MLP architecture:\n",
      "Sequential(\n",
      "  (0): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "Total parameters: 46\n",
      "Parameter containing:\n",
      "tensor([[-0.1406, -0.0798, -0.2810,  0.0465, -0.2609, -0.2853, -0.1765,  0.2476],\n",
      "        [ 0.3275,  0.2455,  0.3048, -0.2343, -0.1381, -0.2571, -0.3449, -0.2552],\n",
      "        [ 0.0582,  0.0205, -0.0140,  0.1112,  0.0567, -0.1402,  0.3515,  0.0824],\n",
      "        [-0.2172,  0.0858,  0.1001, -0.2745,  0.2858,  0.1498,  0.0444, -0.0041]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2968, -0.2372, -0.0278,  0.1049], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.4880, -0.0783, -0.0290, -0.3552],\n",
      "        [-0.0612,  0.0274, -0.0900,  0.1887]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0724, -0.4244], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create a simple 2-layer MLP using nn.Sequential\n",
    "# Input: 8 features -> Hidden: 4 neurons with ReLU -> Output: 2 neurons\n",
    "simple_mlp = nn.Sequential(\n",
    "    nn.Linear(8, 4, device=\"cuda\"),\n",
    "    relu_activation,\n",
    "    nn.Linear(4, 2, device=\"cuda\")\n",
    ")\n",
    "\n",
    "if simple_mlp is not None:\n",
    "    print(\"Simple MLP architecture:\")\n",
    "    print(simple_mlp)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in simple_mlp.parameters())\n",
    "    print(f\"\\nTotal parameters: {total_params}\")\n",
    "    \n",
    "for p in simple_mlp.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep MLP architecture:\n",
      "Sequential(\n",
      "  (0): Linear(in_features=10, out_features=8, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=8, out_features=6, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=6, out_features=4, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "Calculated parameters: 180\n",
      "Actual parameters: 180\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create a deeper MLP with 3 hidden layers\n",
    "# Input: 10 -> Hidden1: 8 (ReLU) -> Hidden2: 6 (ReLU) -> Hidden3: 4 (ReLU) -> Output: 2\n",
    "deep_mlp = nn.Sequential(\n",
    "    nn.Linear(10, 8),\n",
    "    relu_activation,\n",
    "    nn.Linear(8, 6),\n",
    "    relu_activation,\n",
    "    nn.Linear(6, 4),\n",
    "    relu_activation,\n",
    "    nn.Linear(4,2)\n",
    ")\n",
    "# TODO: Calculate the total number of parameters in deep_mlp\n",
    "# Parameters per layer: (input_size * output_size) + output_size\n",
    "# Layer 1: (10 * 8) + 8 = 88\n",
    "# Layer 2: (8 * 6) + 6 = 54\n",
    "# Layer 3: (6 * 4) + 4 = 28\n",
    "# Layer 4: (4 * 2) + 2 = 10\n",
    "deep_mlp_params = sum(p.numel() for p in deep_mlp.parameters())\n",
    "\n",
    "if deep_mlp is not None and deep_mlp_params is not None:\n",
    "    print(\"Deep MLP architecture:\")\n",
    "    print(deep_mlp)\n",
    "    print(f\"\\nCalculated parameters: {deep_mlp_params}\")\n",
    "    actual_params = sum(p.numel() for p in deep_mlp.parameters())\n",
    "    print(f\"Actual parameters: {actual_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 3: Building Networks with nn.Sequential\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE3_SECTIONS[\"Section 3: Building Networks with nn.Sequential\"]]\n",
    "test_runner.test_section(\"Section 3: Building Networks with nn.Sequential\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Forward Pass\n",
    "\n",
    "The forward pass is the process of passing input data through the network to get predictions. Each layer transforms the data sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([3, 8])\n",
      "Output shape: torch.Size([3, 2])\n",
      "Output values:\n",
      "tensor([[-0.0957, -0.3488],\n",
      "        [-0.0114, -0.3837],\n",
      "        [ 0.3321, -0.4881]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Perform a forward pass through simple_mlp\n",
    "# Create input data with batch size 3 and 8 features\n",
    "forward_input = torch.randn(3, 8, device=\"cuda\")\n",
    "\n",
    "# TODO: Pass the input through simple_mlp\n",
    "simple_output = simple_mlp(forward_input)\n",
    "\n",
    "if simple_output is not None:\n",
    "    print(f\"Input shape: {forward_input.shape}\")\n",
    "    print(f\"Output shape: {simple_output.shape}\")\n",
    "    print(f\"Output values:\\n{simple_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6084],\n",
      "        [0.6131],\n",
      "        [0.5860],\n",
      "        [0.6331],\n",
      "        [0.5956]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "Mixed activation MLP:\n",
      "Sequential(\n",
      "  (0): Linear(in_features=6, out_features=4, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=4, out_features=3, bias=True)\n",
      "  (3): Tanh()\n",
      "  (4): Linear(in_features=3, out_features=1, bias=True)\n",
      "  (5): Sigmoid()\n",
      ")\n",
      "\n",
      "Input shape: torch.Size([5, 6])\n",
      "Output shape: torch.Size([5, 1])\n",
      "Output range: [0.5860, 0.6331]\n",
      "(Note: Sigmoid ensures output is between 0 and 1)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create a network with mixed activation functions\n",
    "# Input: 6 -> Hidden1: 4 (ReLU) -> Hidden2: 3 (Tanh) -> Output: 1 (Sigmoid)\n",
    "mixed_activation_mlp = nn.Sequential(\n",
    "    nn.Linear(6, 4),\n",
    "    relu_activation,\n",
    "    nn.Linear(4, 3),\n",
    "    tanh_activation,\n",
    "    nn.Linear(3,1),\n",
    "    sigmoid_activation\n",
    ")\n",
    "\n",
    "# TODO: Perform forward pass with batch size 5\n",
    "mixed_input = torch.randn(5, 6)\n",
    "mixed_output = mixed_activation_mlp(mixed_input)  # Pass mixed_input through mixed_activation_mlp\n",
    "print(mixed_output)\n",
    "if mixed_activation_mlp is not None and mixed_output is not None:\n",
    "    print(\"Mixed activation MLP:\")\n",
    "    print(mixed_activation_mlp)\n",
    "    print(f\"\\nInput shape: {mixed_input.shape}\")\n",
    "    print(f\"Output shape: {mixed_output.shape}\")\n",
    "    print(f\"Output range: [{mixed_output.min().item():.4f}, {mixed_output.max().item():.4f}]\")\n",
    "    print(\"(Note: Sigmoid ensures output is between 0 and 1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 4: Forward Pass\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE3_SECTIONS[\"Section 4: Forward Pass\"]]\n",
    "test_runner.test_section(\"Section 4: Forward Pass\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Understanding Parameter Counting\n",
    "\n",
    "Understanding how many parameters your network has is crucial for model complexity and memory requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple MLP parameters: 46\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create a function to count parameters in any model\n",
    "def count_parameters(model: nn.Module):\n",
    "    \"\"\"\n",
    "    Count the total number of trainable parameters in a model.\n",
    "    \n",
    "    Args:\n",
    "        model: A PyTorch nn.Module\n",
    "    \n",
    "    Returns:\n",
    "        Total number of parameters\n",
    "    \"\"\"\n",
    "    # TODO: Complete this function\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Test your function\n",
    "if count_parameters is not None and simple_mlp is not None:\n",
    "    param_count = count_parameters(simple_mlp)\n",
    "    if param_count is not None:\n",
    "        print(f\"Simple MLP parameters: {param_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO REDONDANT \n",
    "\n",
    "\n",
    "# TODO: Create a large MLP and calculate its parameters\n",
    "# Input: 100 -> Hidden1: 64 -> Hidden2: 32 -> Hidden3: 16 -> Output: 10\n",
    "# Use ReLU activation between layers (except after output)\n",
    "large_mlp = None\n",
    "\n",
    "# TODO: Calculate expected number of parameters manually\n",
    "# Layer 1: (100 * 64) + 64 = ?\n",
    "# Layer 2: (64 * 32) + 32 = ?\n",
    "# Layer 3: (32 * 16) + 16 = ?\n",
    "# Layer 4: (16 * 10) + 10 = ?\n",
    "expected_params = None  # Sum all layer parameters\n",
    "\n",
    "if large_mlp is not None and expected_params is not None and count_parameters is not None:\n",
    "    actual_params = count_parameters(large_mlp)\n",
    "    if actual_params is not None:\n",
    "        print(f\"Expected parameters: {expected_params}\")\n",
    "        print(f\"Actual parameters: {actual_params}\")\n",
    "        print(f\"Match: {expected_params == actual_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Section 5: Understanding Parameter Counting\n",
    "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE3_SECTIONS[\"Section 5: Understanding Parameter Counting\"]]\n",
    "test_runner.test_section(\"Section 5: Understanding Parameter Counting\", validator, section_tests, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final summary of all tests\n",
    "test_runner.final_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
